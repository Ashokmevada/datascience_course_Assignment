{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. Difference between Simple Linear Regression and Multiple Linear Regression:**\n",
    "\n",
    "- **Simple Linear Regression:** This involves a relationship between two variables, where one is the independent variable (predictor) and the other is the dependent variable (response). It assumes a linear relationship between the variables and aims to find the best-fitting line (a straight line) that minimizes the sum of squared differences between the observed and predicted values.\n",
    "\n",
    "Example: Predicting a student's exam score (dependent variable) based on the number of hours they studied (independent variable).\n",
    "\n",
    "- **Multiple Linear Regression:** This extends the concept of simple linear regression to include more than one independent variable. It models the relationship between multiple independent variables and a single dependent variable. The goal is to find a hyperplane that best fits the data in a higher-dimensional space.\n",
    "\n",
    "Example: Predicting house prices (dependent variable) based on factors like square footage, number of bedrooms, and distance to the nearest public transportation (independent variables).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q2. Assumptions of Linear Regression and How to Check Them:**\n",
    "\n",
    "The assumptions of linear regression include linearity, independence of errors, homoscedasticity (constant variance of errors), normality of errors, and lack of multicollinearity.\n",
    "\n",
    "To check these assumptions, you can use various methods such as:\n",
    "- **Residual Plots:** Plot the residuals (differences between observed and predicted values) against the predicted values to check for patterns or heteroscedasticity.\n",
    "- **Normality Tests:** Use tests like the Shapiro-Wilk test or Q-Q plots to assess the normality of residuals.\n",
    "- **Multicollinearity Detection:** Calculate the Variance Inflation Factor (VIF) to identify multicollinearity among independent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q3. Interpreting Slope and Intercept in Linear Regression:**\n",
    "\n",
    "- **Intercept:** It's the value of the dependent variable when all independent variables are zero. It might not always have a meaningful interpretation.\n",
    "\n",
    "- **Slope (Coefficient):** It represents the change in the dependent variable for a one-unit change in the corresponding independent variable, keeping other variables constant.\n",
    "\n",
    "Example: In the context of predicting exam scores, the intercept might represent the baseline score, and the slope of the hours studied variable indicates how much the score is expected to increase for each additional hour studied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q4. Gradient Descent:**\n",
    "\n",
    "Gradient descent is an optimization technique used to find the minimum of a function. In the context of machine learning, it's often used to update the parameters of a model to minimize the difference between predicted and actual values. It iteratively adjusts the model's parameters in the direction of the steepest descent of the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q5. Multiple Linear Regression vs. Simple Linear Regression:**\n",
    "\n",
    "In multiple linear regression, there are multiple independent variables affecting a single dependent variable, allowing for a more complex representation of relationships. The multiple linear regression model's equation includes coefficients for each independent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q6. Multicollinearity:**\n",
    "\n",
    "Multicollinearity refers to high correlations between independent variables in a multiple linear regression model. It can make it difficult to distinguish the individual effects of the correlated variables on the dependent variable. It can be detected using methods like the VIF, and addressed by removing correlated variables or transforming them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q7. Polynomial Regression:**\n",
    "\n",
    "Polynomial regression is an extension of linear regression where the relationship between the independent and dependent variables is modeled as an nth-degree polynomial. This allows for capturing more complex nonlinear relationships between the variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q8. Advantages and Disadvantages of Polynomial Regression:**\n",
    "\n",
    "- **Advantages:** Can model nonlinear relationships, fits data better when a linear model is inadequate, captures curvature in data.\n",
    "\n",
    "- **Disadvantages:** More complex, can lead to overfitting if degree is too high, extrapolation can be unreliable.\n",
    "\n",
    "Use Polynomial Regression when there's evidence that the relationship isn't linear and you're okay with a more complex model, but be cautious of overfitting and consider cross-validation techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
