{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1\n",
    "\n",
    "Min-Max scaling is a data preprocessing technique used to scale the numerical features of a dataset to a specific range, typically between 0 and 1. It works by transforming each feature's values proportionally so that the minimum value becomes 0, and the maximum value becomes 1. The formula for Min-Max scaling is:\n",
    "\n",
    "Scaled_value = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "where X is the original value, X_min is the minimum value of the feature, and X_max is the maximum value of the feature.\n",
    "\n",
    "Example: Let's say we have a dataset of house prices, and the minimum and maximum prices in the dataset are 10,00,000 Rs and 50,00,000, respectively. We want to scale the prices using Min-Max scaling. If a particular house price is 20,00,000 Rs , the scaled value would be:\n",
    "\n",
    "Scaled_value = (2000000 - 1000000 ) / (5000000 - 1000000) = 0.25\n",
    "\n",
    "So, the scaled value for 2000000 would be 0.25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 \n",
    "\n",
    "The Unit Vector technique, also known as normalization or L2 normalization, is a feature scaling method used to transform data such that each data point lies on the surface of a unit circle (i.e., having a magnitude of 1). Unlike Min-Max scaling, it doesn't constrain the data to a specific range but normalizes the data based on the feature vector's magnitude.\n",
    "\n",
    "The formula for Unit Vector scaling is:\n",
    "\n",
    "Scaled_value = X / ||X||\n",
    "\n",
    "where X is the original feature vector, and ||X|| is the Euclidean norm (magnitude) of the feature vector.\n",
    "\n",
    "Example: Consider a dataset with two numerical features: age and income. Let's say a particular data point has age = 40 and income = 60,000 Rs. To perform Unit Vector scaling, we first calculate the magnitude of the feature vector:\n",
    "\n",
    "||X|| = √(40^2 + 60000^2) ≈ 60005.78\n",
    "\n",
    "Then, we calculate the scaled values for age and income:\n",
    "\n",
    "Scaled_age = 40 / 60005.78 ≈ 0.0006666666\n",
    "Scaled_income = 60000 / 60005.78 ≈ 0.9999444464\n",
    "\n",
    "So, the scaled values for age and income would be approximately 0.00067 and 0.99994, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. \n",
    "\n",
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving the most important information in the data. It does this by identifying the principal components, which are orthogonal vectors that represent the directions of maximum variance in the data.\n",
    "\n",
    "In PCA, the first principal component captures the most variance, followed by the second principal component, and so on. By projecting the data onto a subset of these principal components, we can reduce the dimensionality of the data while retaining most of its variance.\n",
    "\n",
    "Example: Let's consider a dataset with three features: height, weight, and age. PCA would analyze the data to find the first principal component, which would be a linear combination of the original features. The first principal component might represent overall body size. The second principal component could represent age-related variations, and the third principal component could capture any remaining variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. \n",
    "\n",
    "PCA can be used for Feature Extraction, which involves transforming the original features into a new set of features (principal components) that represent the data's variability in a more efficient way. This process reduces the number of features while retaining most of the important information.\n",
    "\n",
    "Example: Suppose we have a dataset of images represented by pixel values. Each image is 100x100 pixels, giving us 10,000 features per image. Applying PCA to this dataset would identify the principal components that represent the most significant patterns and variations in the images. We could then choose to retain only the top few principal components (e.g., 100 components) instead of the original 10,000 features. These selected principal components can then be used as the new feature set for further analysis or machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. \n",
    "\n",
    "For the food delivery service recommendation system project, Min-Max scaling can be used to preprocess the numerical features such as price, rating, and delivery time.\n",
    "\n",
    "1. Identify the minimum and maximum values for each numerical feature (e.g., price, rating, delivery time) in the dataset.\n",
    "2. For each feature value X, apply the Min-Max scaling formula:\n",
    "\n",
    "Scaled_value = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "3. Replace the original feature values with the scaled values in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. \n",
    "\n",
    "For the stock price prediction project, PCA can be used to reduce the dimensionality of the dataset containing multiple features related to company financial data and market trends.\n",
    "\n",
    "1. Standardize the numerical features in the dataset (e.g., mean centering and scaling to unit variance) to ensure all features have comparable scales.\n",
    "2. Apply PCA to the standardized dataset to calculate the principal components and their corresponding variances.\n",
    "3. Sort the principal components in descending order of variance to identify the components that explain the most variance in the data.\n",
    "4. Choose the desired number of principal components (retained features) based on the explained variance ratio or a predefined threshold.\n",
    "\n",
    "For example, if the dataset has 20 features, you might decide to retain the top 10 principal components, which represent the most significant patterns and trends in the stock market data. These 10 principal components will serve as the new, lower-dimensional feature set for building the prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Q7. \n",
    "\n",
    "To perform Min-Max scaling on the dataset [1, 5, 10, 15, 20] and transform the values to a range of -1 to 1, follow these steps:\n",
    "\n",
    "1. Identify the minimum and maximum values in the dataset: Min = 1, Max = 20.\n",
    "2. Apply the Min-Max scaling formula for each value:\n",
    "\n",
    "Scaled_value = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "3. Replace the original values with the scaled values.\n",
    "\n",
    "Scaled_values = [(-1 - 1) / (20 - 1), (5 - 1) / (20 - 1), (10 - 1) / (20 - 1), (15 - 1) / (20 - 1), (20 - 1) / (20 - 1)]\n",
    "Scaled_values = [-0.909, 0.182, 0.545, 0.909, 1.000]\n",
    "\n",
    "The Min-Max scaled values in the range of -1 to 1 would be approximately [-0.909, 0.182, 0.545, 0.909, 1.000]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Q8. \n",
    "\n",
    "To perform Feature Extraction using PCA on the dataset [height, weight, age, gender\n",
    "\n",
    ", blood pressure], follow these steps:\n",
    "\n",
    "1. Standardize the numerical features in the dataset (e.g., mean centering and scaling to unit variance) to ensure all features have comparable scales.\n",
    "2. Apply PCA to the standardized dataset to calculate the principal components and their corresponding variances.\n",
    "3. Sort the principal components in descending order of variance to identify the components that explain the most variance in the data.\n",
    "4. Choose the desired number of principal components to retain based on the explained variance ratio or a predefined threshold.\n",
    "\n",
    "The number of principal components to retain depends on the desired level of dimensionality reduction. A common approach is to retain enough principal components to explain a significant portion of the total variance in the data. For example, you may choose to retain principal components that explain at least 95% of the total variance.\n",
    "\n",
    "After selecting the number of principal components, you would project the data onto these components to obtain the reduced feature set, which would be used for further analysis or modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
