{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1\n",
    "\n",
    "Overfitting: Overfitting occurs when a machine learning model performs extremely well on the training data but fails to generalize to new, unseen data. The model becomes too complex and captures noise or random fluctuations in the training data, leading to poor performance on test or validation data.\n",
    "Consequences: Reduced model performance on unseen data, potential loss of generalization, and increased sensitivity to small changes in the training data. \n",
    "\n",
    "Mitigation: Regularization techniques, cross-validation, early stopping, and increasing the size of the training dataset can help reduce overfitting.\n",
    "\n",
    "Underfitting: Underfitting happens when a machine learning model is too simple to capture the underlying patterns in the training data. As a result, the model performs poorly on both training and test data.\n",
    "Consequences: Limited predictive power, poor model performance, and failure to learn the underlying relationships in the data.\n",
    "Mitigation: Using more complex models, adding relevant features, and increasing the size of the training dataset can help mitigate underfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Q2\n",
    "\n",
    "Use a larger and more diverse dataset to improve generalization.\n",
    "Apply regularization techniques (e.g., L1 or L2 regularization) to penalize overly complex models.\n",
    "Employ techniques like dropout in deep learning to prevent over-reliance on specific neurons during training.\n",
    "Perform feature selection to include only the most relevant features.\n",
    "Use cross-validation to assess the model's performance on different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3\n",
    "\n",
    "Underfitting occurs when a model is too simplistic to capture the underlying patterns in the data. It can happen in various scenarios:\n",
    "Using a linear model to fit a highly non-linear relationship between features and target.\n",
    "Insufficient model complexity for a complex problem.\n",
    "Using too few features or not including important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4\n",
    "\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that describes the relationship between bias and variance in model performance. High bias indicates that the model is too simplistic and fails to capture the underlying patterns in the data. High variance indicates that the model is too complex and is sensitive to small fluctuations in the training data. The tradeoff suggests that as you reduce bias (make the model more complex), variance tends to increase, and vice versa.\n",
    "\n",
    "High bias models tend to underfit, while high variance models tend to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5\n",
    "\n",
    "Cross-validation: Evaluate the model's performance on multiple train-test splits to check for consistency.\n",
    "Learning curves: Plot the training and validation performance as a function of the training data size to visualize overfitting or underfitting.\n",
    "Holdout validation set: Use a separate validation set to check for overfitting during training.\n",
    "Regularization parameter tuning: Observe the impact of regularization on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6\n",
    "\n",
    "High Bias (Underfitting) Model: A linear regression model used to predict a highly non-linear relationship between variables.\n",
    "High Variance (Overfitting) Model: A decision tree with too many levels, capturing noise and outliers in the training data.\n",
    "\n",
    "In terms of performance, high bias models have poor training and testing performance, while high variance models have excellent training performance but poor testing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7\n",
    "\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty term to the model's loss function. The penalty discourages large coefficient values, making the model less complex.\n",
    "\n",
    "Common regularization techniques:\n",
    "1. L1 regularization (Lasso): Adds the absolute value of coefficients as a penalty term.\n",
    "2. L2 regularization (Ridge): Adds the squared value of coefficients as a penalty term.\n",
    "3. Dropout (for deep learning): Randomly drops neurons during training to prevent over-reliance on specific neurons.\n",
    "4. Early stopping: Stop training the model when the validation performance stops improving.\n",
    "\n",
    "Regularization helps to simplify models and reduce overfitting, leading to improved generalization performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
