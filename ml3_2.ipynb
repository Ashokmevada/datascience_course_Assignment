{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. R-squared in Linear Regression:**\n",
    "\n",
    "R-squared (coefficient of determination) is a statistical metric that measures the proportion of the variance in the dependent variable (response) that is explained by the independent variables (predictors) in a linear regression model. It's a value between 0 and 1, where 0 indicates that the model explains none of the variance and 1 indicates that the model explains all of the variance.\n",
    "\n",
    "R-squared is calculated as the ratio of the explained variance to the total variance of the dependent variable. Mathematically, it can be expressed as: \n",
    "\n",
    "\\[ R^2 = \\frac{\\text{Explained Variance}}{\\text{Total Variance}} = 1 - \\frac{\\text{Residual Sum of Squares}}{\\text{Total Sum of Squares}} \\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. Adjusted R-squared:**\n",
    "\n",
    "Adjusted R-squared is a modification of R-squared that accounts for the number of predictors in the model. It penalizes the inclusion of unnecessary variables that might not contribute significantly to explaining the variance. Adjusted R-squared increases only if the added predictor improves the model more than what would be expected by random chance.\n",
    "\n",
    "Adjusted R-squared is calculated using the formula:\n",
    "\n",
    "\\[ \\text{Adjusted R}^2 = 1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1} \\]\n",
    "\n",
    "where \\( n \\) is the number of observations and \\( p \\) is the number of predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q3. Use of Adjusted R-squared:**\n",
    "\n",
    "Adjusted R-squared is more appropriate when comparing models with different numbers of predictors. It helps to balance between model complexity and goodness of fit. As you add more predictors to a model, the regular R-squared may artificially increase, even if the additional predictors don't contribute much to the improvement of the model. Adjusted R-squared accounts for this and provides a better measure of model quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q4. RMSE, MSE, and MAE:**\n",
    "\n",
    "- **RMSE (Root Mean Squared Error):** It's the square root of the average of the squared differences between the predicted and actual values. It gives more weight to larger errors.\n",
    "\n",
    "- **MSE (Mean Squared Error):** It's the average of the squared differences between predicted and actual values. It's useful for understanding the average magnitude of errors.\n",
    "\n",
    "- **MAE (Mean Absolute Error):** It's the average of the absolute differences between predicted and actual values. It treats all errors equally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q5. Advantages and Disadvantages of RMSE, MSE, and MAE:**\n",
    "\n",
    "- **Advantages:** They provide a quantitative measure of prediction error, easy to interpret, sensitive to outliers (RMSE and MSE), and more robust to outliers (MAE).\n",
    "\n",
    "- **Disadvantages:** Sensitive to outliers (RMSE and MSE), may not always provide intuitive understanding of errors (MSE), and MAE doesn't differentiate between large and small errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q6. Lasso Regularization:**\n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used to prevent overfitting in linear regression models by adding a penalty term to the loss function. It adds the absolute value of the coefficients as a penalty, which can lead to some coefficients becoming exactly zero. This encourages feature selection and produces sparse models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q7. Regularized Linear Models and Overfitting:**\n",
    "\n",
    "Regularized linear models, such as Ridge and Lasso regression, add a regularization term to the loss function that penalizes large coefficients. This prevents the model from fitting noise in the data, reducing overfitting. For example, in Ridge regression, the L2 regularization term limits the magnitude of coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q8. Limitations of Regularized Linear Models:**\n",
    "\n",
    "Regularization might not always be appropriate if the relationships between variables are truly complex and require high-dimensional interactions. Also, regularization doesn't guarantee the elimination of all overfitting, and tuning the regularization parameter can be challenging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q9. Choosing Model Based on RMSE and MAE:**\n",
    "\n",
    "Both RMSE and MAE measure different aspects of error. RMSE is more sensitive to outliers, whereas MAE treats all errors equally. In this case, Model B with an MAE of 8 would be chosen as the better performer. However, the choice depends on the problem and the importance of different types of errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q10. Choosing Regularization Method:**\n",
    "\n",
    "Choosing between Ridge and Lasso regularization depends on the problem. Ridge tends to work better when most features are relevant and you want to shrink their coefficients, while Lasso performs feature selection by driving some coefficients to zero. In the given scenario, Model A with Ridge regularization might be chosen if preserving most features is important.\n",
    "\n",
    "Remember that the choice of regularization depends on the problem and the underlying characteristics of the data. There's often a trade-off between model complexity, interpretability, and predictive performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
