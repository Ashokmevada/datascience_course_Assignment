{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "The mathematical formula for a linear SVM is given by the equation of a hyperplane:  \n",
    "\n",
    "f(x)=sign(wâ‹…x+b)\n",
    "\n",
    "where:\n",
    "\n",
    "f(x) is the decision function that predicts the class of the input \n",
    "\n",
    "w is the weight vector.\n",
    "\n",
    "x is the input feature vector.\n",
    "\n",
    "b is the bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q2. What is the objective function of a linear SVM?**\n",
    "The objective function of a linear SVM aims to maximize the margin between the two classes while minimizing the classification error. \n",
    "\n",
    "\n",
    "**Q3. What is the kernel trick in SVM?**\n",
    "The kernel trick is a technique used in SVMs to implicitly map the input data into a higher-dimensional space without explicitly computing the transformation. This allows SVMs to efficiently handle non-linearly separable data. Popular kernel functions include the linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
    "\n",
    "**Q4. What is the role of support vectors in SVM? Explain with an example.**\n",
    "Support vectors are the data points that are closest to the decision boundary (margin) and play a crucial role in determining the position of the decision boundary. These points support the construction of the hyperplane. In a linear SVM, only the support vectors influence the decision boundary, making SVMs memory-efficient and robust to outliers.\n",
    "\n",
    "Example: Imagine a binary classification problem with two well-separated classes. The support vectors would be the few points from each class that are closest to the other class. These points dictate the position and orientation of the hyperplane that separates the two classes.\n",
    "\n",
    "**Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM?**\n",
    "\n",
    "- **Hyperplane:** In a 2D dataset, a hyperplane is a straight line that separates two classes. In higher dimensions, it becomes a hyperplane.\n",
    "\n",
    "- **Margin and Marginal Plane:** The margin is the region between two parallel hyperplanes that are equidistant from the support vectors. The marginal plane is the midplane of the margin, and the goal of SVM is to find the optimal marginal plane that maximizes the margin.\n",
    "\n",
    "- **Hard Margin SVM:** In a hard margin SVM, the goal is to find a hyperplane that perfectly separates the classes without any misclassification. It's suitable for linearly separable datasets, but sensitive to outliers.\n",
    "\n",
    "- **Soft Margin SVM:** In a soft margin SVM, a small amount of misclassification is allowed to achieve a better generalization to slightly non-linearly separable datasets. A regularization parameter (C) controls the trade-off between maximizing the margin and minimizing the classification error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (C=0.01): 0.97\n",
      "Accuracy (C=1): 1.00\n",
      "Accuracy (C=100): 1.00\n"
     ]
    }
   ],
   "source": [
    "# Q6 :\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:4]  # Using only two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "C_values = [0.01, 1, 100]  # Different values of regularization parameter C\n",
    "for C in C_values:\n",
    "    svm_clf = SVC(C=C, kernel='linear')\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict labels for the testing set\n",
    "    y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(f\"Accuracy (C={C}): {accuracy:.2f}\")\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
